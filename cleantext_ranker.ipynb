{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "cleantext_ranker.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "5TFyqUQet9m6",
        "5LyPs7e_AyCm",
        "qbA-vVFT-oUD",
        "EHn-KZ98urY_",
        "gxzOvQ_-2aSl"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WXAVj_jDPqSa",
        "outputId": "e382d492-a038-4e73-df66-6aeeaf689b94"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-XZXriOmRGF1",
        "outputId": "a1570309-0deb-4c99-c1f3-9a759127ed02"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "NVIDIA-SMI has failed because it couldn't communicate with the NVIDIA driver. Make sure that the latest NVIDIA driver is installed and running.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VO_cHETDQHR0"
      },
      "source": [
        "import os\n",
        "import pickle\n",
        "import random\n",
        "from collections import Counter\n",
        "import time\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "## pytorch libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "\n",
        "import torchtext\n",
        "from torchtext import data\n",
        "from torchtext.datasets import YahooAnswers\n",
        "from torchtext.data import get_tokenizer\n",
        "\n",
        "from torchtext.vocab import vocab, GloVe\n",
        "\n",
        "\n",
        "import spacy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "uuGugFgbQSWq",
        "outputId": "bc0edf13-6133-4932-db31-519ef847791b"
      },
      "source": [
        "os.getcwd()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content'"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5TFyqUQet9m6"
      },
      "source": [
        "# Load data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4p6ULUoY6fFx"
      },
      "source": [
        "Original dataset separates the topic (question asked), text of the opening post, and the most highly voted answer. However, pytorch includes all this information in one string. Can we improve accuracy by only using the topic title?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rDlRRHtBC9cG"
      },
      "source": [
        "Load original .csv datasets\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0hZm7RmIHTad"
      },
      "source": [
        "data_path = '/content/drive/My Drive/ARP/data'\n",
        "\n",
        "train_csv = data_path + '/train.csv'\n",
        "test_csv = data_path + '/test.csv'\n",
        "\n",
        "train_df = pd.read_csv(train_csv, header = None)\n",
        "test_df = pd.read_csv(test_csv, header = None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iqaa_4nUH6gQ"
      },
      "source": [
        "# extract relevant columns: label and topic (first and second columns)\n",
        "\n",
        "train_data = [(label, topic) for label, topic \n",
        "              in zip(train_df[0].to_list(), train_df[1].to_list())]\n",
        "\n",
        "test_data = [(label, topic) for label, topic \n",
        "              in zip(test_df[0].to_list(), test_df[1].to_list())]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wIAb3DIy6azo"
      },
      "source": [
        "### Split data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QMYBGM23wCPt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ae0206e5-8ef7-49e3-938c-2945c168051c"
      },
      "source": [
        "## original train dataset is very large: \n",
        "print(len(train_data))\n",
        "\n",
        "# for prototyping purposes, we will only use 5% of this data to improve train\n",
        "# times\n",
        "train_keep, train_discard = train_test_split(train_data, train_size = 0.05, \n",
        "                                        random_state = 123)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1400000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZrR2WQONPl-r"
      },
      "source": [
        "# further divide the training set into train and validation set\n",
        "# 70% train, 30% validation\n",
        "\n",
        "train_data, val_data = train_test_split(train_keep, train_size = 0.7, \n",
        "                                        random_state = 123)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ESTCvaLFSUtI"
      },
      "source": [
        "Review data splits"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r0U8mcZNAs0L",
        "outputId": "e9bf8647-e443-4f9f-afba-a5116f06af2c"
      },
      "source": [
        "train_data[0:5]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(9,\n",
              "  'im gay and i like this girl but shes taken and i really like her but im still in luv with my ex g/f i just don'),\n",
              " (8, 'Who was the best in American Idol tonight?'),\n",
              " (3,\n",
              "  'How can I cure excessive underarm sweat, I have tried all deodrants nothing works please help!?'),\n",
              " (2,\n",
              "  'why we substuite assay by titrations caliculation molicularweight/1000?'),\n",
              " (4,\n",
              "  'what if you know your teacher likes you, but she has a temper and does\\'t like \"dumb questions\"?')]"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mPyFMaGpAw7j",
        "outputId": "62b48886-dae3-44f1-db62-25532ed351fb"
      },
      "source": [
        "val_data[0:5]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(4, 'Why dont we use class 5 IP addresses?'),\n",
              " (10, \"what if can't find voters card to vote?\"),\n",
              " (10, 'Do you believe the war on terror is genuine?'),\n",
              " (4, 'what does \"targa\" mean?'),\n",
              " (3, 'I was just diagnosed HIV +. What should I do now?')]"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O7vBRQ0hAxCN",
        "outputId": "2b75e3e3-7a42-49c5-b988-3053bb9f285c"
      },
      "source": [
        "test_data[0:5]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(9, 'What makes friendship click?'),\n",
              " (2, 'Why does Zebras have stripes?'),\n",
              " (4, 'What did the itsy bitsy sipder climb up?'),\n",
              " (4, 'What is the difference between a Bachelors and a Masters degree?'),\n",
              " (3, 'Why do women get PMS?')]"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Jxd40NhSPyM",
        "outputId": "bbc01fcd-8d0c-4108-f6f1-509f8203a88e"
      },
      "source": [
        "print(f'Train instances: {len(train_data)}')\n",
        "print(f'Val instances: {len(val_data)}')\n",
        "print(f'Test instances: {len(test_data)}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train instances: 49000\n",
            "Val instances: 21000\n",
            "Test instances: 60000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5LyPs7e_AyCm"
      },
      "source": [
        "# Build data processing pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w_PChIg20pfM"
      },
      "source": [
        "Beginning of Pytorch pipeline. The following neural network architecture is based on https://github.com/bentrevett/pytorch-sentiment-analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V_4dKIWaz88u"
      },
      "source": [
        "Set up tokeniser - in this case, will use Spacy and large language model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Vduxxvy7DJ7",
        "outputId": "be25945e-ced8-47c6-9cee-073e68886f19"
      },
      "source": [
        "# download Spacy large English language model into google colab environment\n",
        "\n",
        "!python -m spacy download en_core_web_lg"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting en_core_web_lg==2.2.5\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-2.2.5/en_core_web_lg-2.2.5.tar.gz (827.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 827.9 MB 1.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.7/dist-packages (from en_core_web_lg==2.2.5) (2.2.4)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (4.62.0)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (3.0.5)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (2.23.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (2.0.5)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.19.5)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (7.4.0)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (0.8.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (57.4.0)\n",
            "Requirement already satisfied: importlib-metadata>=0.20 in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_lg==2.2.5) (4.6.4)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_lg==2.2.5) (3.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_lg==2.2.5) (3.7.4.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_lg==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_lg==2.2.5) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_lg==2.2.5) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_lg==2.2.5) (2021.5.30)\n",
            "Building wheels for collected packages: en-core-web-lg\n",
            "  Building wheel for en-core-web-lg (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for en-core-web-lg: filename=en_core_web_lg-2.2.5-py3-none-any.whl size=829180942 sha256=3eaa2069d0b4d738ab4335a53076562905848a74eb8417fca5d1f5b24d765405\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-ao1a2t1k/wheels/11/95/ba/2c36cc368c0bd339b44a791c2c1881a1fb714b78c29a4cb8f5\n",
            "Successfully built en-core-web-lg\n",
            "Installing collected packages: en-core-web-lg\n",
            "Successfully installed en-core-web-lg-2.2.5\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_lg')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SQZsaf3T-pVo",
        "outputId": "4bb20f77-35fc-4304-b7a9-ee8ee758b01e"
      },
      "source": [
        "#Link alias 'en' to large language english model\n",
        "\n",
        "!python -m spacy link en_core_web_lg en --force"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[38;5;2m✔ Linking successful\u001b[0m\n",
            "/usr/local/lib/python3.7/dist-packages/en_core_web_lg -->\n",
            "/usr/local/lib/python3.7/dist-packages/spacy/data/en\n",
            "You can now load the model via spacy.load('en')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "jiMDPB5L_0C8",
        "outputId": "063e098e-8c39-4d43-abae-2dbbe584dc62"
      },
      "source": [
        "#checking to ensure the linkage works\n",
        "\n",
        "\n",
        "nlp = spacy.load('en')\n",
        "nlp.meta['name']"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'core_web_lg'"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qbA-vVFT-oUD"
      },
      "source": [
        "# Set up data processing pipeline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8EHpoSDUqrWz",
        "outputId": "22479b29-c229-441b-b161-9d530a58a27e"
      },
      "source": [
        "## set up pytorch device\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cpu\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IbSyPOGA6G4o"
      },
      "source": [
        "# use spacy tokenizer\n",
        "tokenizer = get_tokenizer('spacy', language = 'en')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Sqzj8QIEUK3"
      },
      "source": [
        "# build vocabulary of corpus\n",
        "\n",
        "\n",
        "#tokenize all questions in train dataset\n",
        "counter = Counter()\n",
        "\n",
        "for (label, line) in train_data:\n",
        "  counter.update(tokenizer(line))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pfU7netyFCuR",
        "outputId": "f269c508-b61b-4701-8008-1f5087adc31e"
      },
      "source": [
        "# add minimum frequency required to be considered in vocabulary\n",
        "# Lets say minimum of 20 occurences to be considered in the vocabulary\n",
        "\n",
        "clean_counter = {token: counter[token] for token in counter if counter[token] >= 20}\n",
        "min(clean_counter.values())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "20"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vyn1tqQN4K0p"
      },
      "source": [
        "#construct vocabulary\n",
        "vocabulary = vocab(clean_counter, min_freq = 10) #fairly large corpus, only include words which appear at least 10 times"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DUZjxQYPJvPX"
      },
      "source": [
        "# add <unk> token for out of vocab words and corresponding default index\n",
        "\n",
        "unk_token = '<unk>'\n",
        "\n",
        "vocabulary.insert_token(unk_token, 0)\n",
        "vocabulary.set_default_index(vocabulary[unk_token])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R-_mAdMiw5ki",
        "outputId": "d0f9c4f9-5b9f-413f-e106-2327ebc2016e"
      },
      "source": [
        "#get vocabulary size\n",
        "len(vocabulary)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2322"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EHn-KZ98urY_"
      },
      "source": [
        "## Set up batching"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TBa9EVag2jlR"
      },
      "source": [
        "Set up batching iterator"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mxj8ZVXDcz26"
      },
      "source": [
        "Custom functions in this section are adapted from https://colab.research.google.com/github/pytorch/text/blob/master/examples/legacy_tutorial/migration_tutorial.ipynb"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d0nvEnJzcs0z"
      },
      "source": [
        "# define a custom lambda function used for tokenising text:\n",
        "\n",
        "\n",
        "text_transform = lambda x: [vocabulary[token] for token in tokenizer(x)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0FCD_d9NnKB6"
      },
      "source": [
        "#add <pad> token to vocabulary at specified index\n",
        "\n",
        "pad_token = '<pad>'\n",
        "\n",
        "vocabulary.append_token(pad_token)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LdeK35BgwGEg",
        "outputId": "1f5b1607-849a-47a5-ede9-daeb5ffbc063"
      },
      "source": [
        "vocabulary.__getitem__(pad_token)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2322"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yjnJ6qNA85fE"
      },
      "source": [
        "def collate_batch(batch):\n",
        "  label_list, text_list = [], []\n",
        "\n",
        "  for (label, text) in batch:\n",
        "    label_list.append(label-1) #original data is labelled 1-10; this rescales to 0-9\n",
        "    processed_text = torch.tensor(text_transform(text))\n",
        "    text_list.append(processed_text)\n",
        "\n",
        "  label_out = torch.tensor(label_list)\n",
        "\n",
        "  #label_out =  nn.functional.one_hot(label_tensor, 10) #one hot encode target variable\n",
        "  #label_out = label_out.type(torch.float)\n",
        "\n",
        "  text_out = pad_sequence(text_list, padding_value= vocabulary.__getitem__(pad_token))\n",
        "\n",
        "  return label_out.to(device), text_out.to(device)\n",
        "  \n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e7K9XgH1flf0"
      },
      "source": [
        "define custom fiunction to replicate behaviour from BucketIterator from legacy Pytorch - batch sentences with similar lengths together"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wuJwQXpnftP4"
      },
      "source": [
        "# DEPRECIATED\n",
        "\n",
        "\n",
        "# def batch_sampler(data):\n",
        "#   indices = [(i, len(tokenizer(s[1]))) for i, s in enumerate(data)] #for each index, get length of sentence\n",
        "#   random.shuffle(indices)\n",
        "#   pooled_indices = []\n",
        "\n",
        "#   #group together indices with similar length\n",
        "#   for i in range(0, len(indices), batch_size * 100):\n",
        "#     pooled_indices.extend(sorted(indices[i:i + batch_size * 100], key=lambda x: x[1]))\n",
        "\n",
        "#   pooled_indices = [x[0] for x in pooled_indices]\n",
        "\n",
        "#   # yield indices for current batch\n",
        "#   for i in range(0, len(pooled_indices), batch_size): #will step up according to batch size\n",
        "#     yield pooled_indices[i:i + batch_size]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S7_AGIbUfuSU"
      },
      "source": [
        "# Using results from Yin et al, using 200 batches seems optimal. \n",
        "# We run initial experiments using this value\n",
        "# batch_size = int(np.ceil(len(train_data) / 200))\n",
        "# print(batch_size)\n",
        "\n",
        "# ^ batch sizes above used up too much memory. Lets default to using the \n",
        "# batch size = 40 used in the paper\n",
        "\n",
        "batch_size = 40\n",
        "\n",
        "# load training, validation and test data into pytorch pipeline.\n",
        "# batching and preprocessing is done using these custom functions\n",
        "\n",
        "train_bloader = DataLoader(train_data, batch_size = batch_size,\n",
        "                           collate_fn = collate_batch)\n",
        "\n",
        "val_bloader = DataLoader(val_data, batch_size = batch_size,\n",
        "                         collate_fn = collate_batch)\n",
        "\n",
        "test_bloader = DataLoader(test_data, batch_size = batch_size,\n",
        "                          collate_fn = collate_batch)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mDUmvMD-3AUE"
      },
      "source": [
        "Build the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5OLmz3y63Fgi"
      },
      "source": [
        "Model follows LSTM architecture from https://github.com/bentrevett/pytorch-sentiment-analysis. \n",
        "Specifically \"2 - Upgraded Sentiment Analysis\"\n",
        "\n",
        "Edits to this architecture were made so it may be used in multi-class classification, as described in \"5 - Multi - Class Sentiment Analysis\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "buaTBpXu259H"
      },
      "source": [
        "class RNN(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_size, output_dim, n_layers, \n",
        "                 bidirectional, dropout, pad_idx):\n",
        "        \n",
        "        super().__init__()\n",
        "        \n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx = pad_idx)\n",
        "        \n",
        "        #bidirectional rnn:processing words in  a sentence both forward and backward\n",
        "        self.rnn = nn.LSTM(embedding_dim, \n",
        "                           hidden_size, \n",
        "                           num_layers=n_layers, \n",
        "                           bidirectional=bidirectional, \n",
        "                           dropout=dropout)\n",
        "        \n",
        "        # final linear layer takes hidden state from both a forward and backwards pass\n",
        "        self.fc = nn.Linear(hidden_size * 2, output_dim) #as many output dimensions as there are classes\n",
        "        \n",
        "        #probability of dropping each neuron\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self, text):\n",
        "        \n",
        "        #text = [sent len, batch size]\n",
        "        \n",
        "        embedded = self.dropout(self.embedding(text))\n",
        "        \n",
        "        #embedded = [sent len, batch size, emb dim]\n",
        "        \n",
        "        #pack sequence\n",
        "        # lengths need to be on CPU!\n",
        "\n",
        "        #packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded)\n",
        "        \n",
        "        output, (hidden, cell) = self.rnn(embedded)\n",
        "        \n",
        "        #unpack sequence\n",
        "        \n",
        "        #output = nn.utils.rnn.pad_packed_sequence(packed_output)\n",
        "\n",
        "        \n",
        "\n",
        "        #output = [sent len, batch size, hid dim * num directions]\n",
        "        #output over padding tokens are zero tensors\n",
        "        \n",
        "       \n",
        "        #cell = [num layers * num directions, batch size, hid dim]\n",
        "\n",
        "\n",
        "        #lstm outputs a tensor witth dimensions:\n",
        "        #hidden = [num layers * num directions, batch size, hid dim]\n",
        "        # alternatively: [forward_layer_0, backward_layer_0, forward_layer_1, \n",
        "        #                 backward_layer_1.....,forward_layer_n, backward_layer_n]\n",
        "\n",
        "        #we want the final FORWARD hidden state and the last BACKWARD hidden state\n",
        "        \n",
        "        #concat the final forward (hidden[-2,:,:]) and backward (hidden[-1,:,:]) hidden layers\n",
        "        #and apply dropout\n",
        "\n",
        "        hidden = self.dropout(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1))\n",
        "                \n",
        "        #hidden = [batch size, hid dim * num directions]\n",
        "\n",
        "        return self.fc(hidden)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uD1DG2h_rMOI"
      },
      "source": [
        "# input model hyperparameters\n",
        "\n",
        "input_dim = len(vocabulary)\n",
        "embedding_dim = 100\n",
        "hidden_size = 20 # from Yin et.al\n",
        "output_dim = 10 #data contains 10 possible classes\n",
        "n_layers = 2\n",
        "dropout = 0.25\n",
        "pad_idx = vocabulary.__getitem__('<pad>')\n",
        "bidirectional = True\n",
        "\n",
        "model = RNN(input_dim, embedding_dim, hidden_size, output_dim,\n",
        "            n_layers, bidirectional, dropout, pad_idx)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "POl1exu96T-c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e510d5ee-c426-4709-e99a-2e969bdb8805"
      },
      "source": [
        "# get pretrained word embeddings for vocabulary\n",
        "\n",
        "vec = GloVe(name = '6B', dim = 100)\n",
        "embed = vec.get_vecs_by_tokens(vocabulary.get_itos())\n",
        "embed"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ".vector_cache/glove.6B.zip: 862MB [02:42, 5.30MB/s]                           \n",
            "100%|█████████▉| 399999/400000 [00:21<00:00, 18873.70it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "        [-0.0465,  0.6197,  0.5665,  ..., -0.3762, -0.0325,  0.8062],\n",
              "        [ 0.2949,  0.5687, -0.2025,  ..., -0.1688,  0.5189, -0.5009],\n",
              "        ...,\n",
              "        [ 0.6308,  0.1315,  0.0275,  ...,  0.0242,  0.3203, -0.2427],\n",
              "        [ 0.1064,  0.0174,  0.8035,  ...,  0.2175,  0.3711, -0.6778],\n",
              "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]])"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cjkbpLdNsEMl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0831f0eb-3849-4477-831d-4fb037e4d705"
      },
      "source": [
        "# set model embeddings to pretrained embeddings\n",
        "\n",
        "model.embedding.weight.data.copy_(embed)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "        [-0.0465,  0.6197,  0.5665,  ..., -0.3762, -0.0325,  0.8062],\n",
              "        [ 0.2949,  0.5687, -0.2025,  ..., -0.1688,  0.5189, -0.5009],\n",
              "        ...,\n",
              "        [ 0.6308,  0.1315,  0.0275,  ...,  0.0242,  0.3203, -0.2427],\n",
              "        [ 0.1064,  0.0174,  0.8035,  ...,  0.2175,  0.3711, -0.6778],\n",
              "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]])"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1BD_atRb0Pbh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1543a071-de08-479c-9aed-d1f7409593b4"
      },
      "source": [
        "#check to ensure that the <pad> and <unk> tokens have been initialised as zeroes\n",
        "print(model.embedding.weight.data[0])\n",
        "print(model.embedding.weight.data[-1]) #pad token was stored as last embedding"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0.])\n",
            "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0.])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sDUcOcnKsKhK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "281e011f-8959-480b-e3d0-a637376befe9"
      },
      "source": [
        "# set up optimizer and loss function\n",
        "\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "model = model.to(device)\n",
        "criterion = criterion.to(device)\n",
        "\n",
        "print(device)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CDGUUGLxscQ9"
      },
      "source": [
        "# define accuracy function\n",
        "\n",
        "def categorical_accuracy(preds, y):\n",
        "  top_pred = preds.argmax(1, keepdim = True) #for each element in the batch, what is the index with the highest output?\n",
        "  correct = top_pred.eq(y.view_as(top_pred)).sum() #how many times did this predictions = correct label\n",
        "  acc = correct.float() / y.shape[0] #average over entire batch\n",
        "  return acc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7I_D94C2wgxH"
      },
      "source": [
        "# set up training function \n",
        "def train(model, iterator, optimizer, criterion):\n",
        "\n",
        "  model.train()\n",
        "\n",
        "  epoch_loss = 0\n",
        "  epoch_acc = 0\n",
        "\n",
        "  #number of batches in iterator to calculate average\n",
        "  n_batch = np.ceil(len(iterator.dataset) / batch_size) #in the case it is not divisible, round up  \n",
        "\n",
        "  for idx, (label, text) in enumerate(iterator):\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    predictions = model(text)\n",
        "\n",
        "    loss = criterion(predictions, label)\n",
        "    #softmax function here?\n",
        "\n",
        "    acc = categorical_accuracy(predictions, label)\n",
        "\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    epoch_loss += loss.item()\n",
        "    epoch_acc += acc.item()\n",
        "\n",
        "  #return average loss and accuracy over all batches\n",
        "  return epoch_loss / n_batch, epoch_acc / n_batch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HinrZq2OyIct"
      },
      "source": [
        "# set up evaluate function\n",
        "\n",
        "def evaluate(model, iterator, criterion):\n",
        "\n",
        "  model.eval()\n",
        "\n",
        "  epoch_loss = 0\n",
        "  epoch_acc = 0\n",
        "\n",
        "  #number of batches in iterator to calculate average\n",
        "  n_batch = np.ceil(len(iterator.dataset) / batch_size) #in the case it is not divisible, round u  \n",
        "\n",
        "\n",
        "  with torch.no_grad():\n",
        "\n",
        "    for idx, (label, text) in enumerate(iterator):\n",
        "\n",
        "      predictions = model(text)     \n",
        "\n",
        "      loss = criterion(predictions, label)\n",
        "      #softmax function here?\n",
        "\n",
        "      acc = categorical_accuracy(predictions, label)\n",
        "\n",
        "      epoch_loss += loss.item()\n",
        "      epoch_acc += acc.item()\n",
        "\n",
        "      \n",
        "  return epoch_loss / float(n_batch), epoch_acc / n_batch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sr_aOqvx2RzN"
      },
      "source": [
        "def epoch_time(start_time, end_time):\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "    return elapsed_mins, elapsed_secs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-2CQR3NOxmzN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "869174b2-f4f9-4898-9490-7379bd1b085b"
      },
      "source": [
        "## get number of trainable parameters\n",
        "\n",
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "\n",
        "count_parameters(model)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "262150"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gxzOvQ_-2aSl"
      },
      "source": [
        "# Train model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QFDI4c1p2Uuf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0d39233c-e0e6-489a-f584-d185f3816b56"
      },
      "source": [
        "n_epochs = 5\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "\n",
        "    save_path = f'/content/drive/My Drive/ARP/model/cleantext_model_{epoch}.pt'\n",
        "\n",
        "    start_time = time.time()\n",
        "    \n",
        "    train_loss, train_acc = train(model, train_bloader, optimizer, criterion)\n",
        "    valid_loss, valid_acc = evaluate(model, val_bloader, criterion)\n",
        "    \n",
        "    end_time = time.time()\n",
        "\n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "\n",
        "    torch.save(model.state_dict(), save_path)\n",
        "    \n",
        "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
        "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 01 | Epoch Time: 0m 40s\n",
            "\tTrain Loss: 1.918 | Train Acc: 33.38%\n",
            "\t Val. Loss: 1.642 |  Val. Acc: 45.65%\n",
            "Epoch: 02 | Epoch Time: 0m 39s\n",
            "\tTrain Loss: 1.565 | Train Acc: 49.29%\n",
            "\t Val. Loss: 1.501 |  Val. Acc: 50.95%\n",
            "Epoch: 03 | Epoch Time: 0m 40s\n",
            "\tTrain Loss: 1.466 | Train Acc: 52.71%\n",
            "\t Val. Loss: 1.465 |  Val. Acc: 52.21%\n",
            "Epoch: 04 | Epoch Time: 0m 40s\n",
            "\tTrain Loss: 1.417 | Train Acc: 54.30%\n",
            "\t Val. Loss: 1.440 |  Val. Acc: 52.84%\n",
            "Epoch: 05 | Epoch Time: 0m 39s\n",
            "\tTrain Loss: 1.382 | Train Acc: 55.23%\n",
            "\t Val. Loss: 1.431 |  Val. Acc: 53.40%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "66ZuEC0PJwLj",
        "outputId": "58f53509-f7ca-4c4b-c29a-a1511350ff1f"
      },
      "source": [
        "# additional 5 epochs of training\n",
        "n_epochs = 10\n",
        "\n",
        "for epoch in range(5, n_epochs):\n",
        "\n",
        "    save_path = f'/content/drive/My Drive/ARP/model/cleantext_model_{epoch}.pt'\n",
        "\n",
        "    start_time = time.time()\n",
        "    \n",
        "    train_loss, train_acc = train(model, train_bloader, optimizer, criterion)\n",
        "    valid_loss, valid_acc = evaluate(model, val_bloader, criterion)\n",
        "    \n",
        "    end_time = time.time()\n",
        "\n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "\n",
        "    torch.save(model.state_dict(), save_path)\n",
        "    \n",
        "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
        "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 06 | Epoch Time: 0m 40s\n",
            "\tTrain Loss: 1.356 | Train Acc: 56.29%\n",
            "\t Val. Loss: 1.434 |  Val. Acc: 53.30%\n",
            "Epoch: 07 | Epoch Time: 0m 39s\n",
            "\tTrain Loss: 1.337 | Train Acc: 56.81%\n",
            "\t Val. Loss: 1.434 |  Val. Acc: 53.14%\n",
            "Epoch: 08 | Epoch Time: 0m 39s\n",
            "\tTrain Loss: 1.317 | Train Acc: 57.16%\n",
            "\t Val. Loss: 1.442 |  Val. Acc: 53.50%\n",
            "Epoch: 09 | Epoch Time: 0m 40s\n",
            "\tTrain Loss: 1.299 | Train Acc: 57.78%\n",
            "\t Val. Loss: 1.444 |  Val. Acc: 53.34%\n",
            "Epoch: 10 | Epoch Time: 0m 40s\n",
            "\tTrain Loss: 1.285 | Train Acc: 58.30%\n",
            "\t Val. Loss: 1.440 |  Val. Acc: 53.29%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XX5IAjoqu5zC"
      },
      "source": [
        "# Evaluate model on test set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tlwe0SC5u8_R"
      },
      "source": [
        "# load model with highest accuracy on validation set\n",
        "\n",
        "# best performing model was trained in epoch 8\n",
        "\n",
        "# path to model\n",
        "model_path = '/content/drive/My Drive/ARP/model/cleantext_model_8.pt'\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "  model_dict = torch.load(model_path)\n",
        "  model.load_state_dict(model_dict)\n",
        "\n",
        "else:\n",
        "  model_dict = torch.load(model_path, map_location = torch.device('cpu'))\n",
        "  model.load_state_dict(model_dict)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hwlRI0hD4L1h"
      },
      "source": [
        "### Evaluate performance on text set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B0Pd4QI04OPx",
        "outputId": "85c83bf6-dbb0-42bd-d79f-4db2587b47f8"
      },
      "source": [
        "model.eval()\n",
        "\n",
        "test_loss, test_acc = evaluate(model, test_bloader, criterion)\n",
        "\n",
        "print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test Loss: 1.441 | Test Acc: 53.52%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kmgMTFB40gQS"
      },
      "source": [
        "Construct function to classify new text instances"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RtuBYN6x0UHX"
      },
      "source": [
        "def predict_class(model, sentence, min_len = 4):\n",
        "\n",
        "  model.eval()\n",
        "\n",
        "  classes = ['Society & Culture',\n",
        "           'Science & Mathematics',\n",
        "           'Health',\n",
        "           'Education & Reference',\n",
        "           'Computers & Internet',\n",
        "           'Sports',\n",
        "           'Business & Finance',\n",
        "           'Entertainment & Music',\n",
        "           'Family & Relationships',\n",
        "           'Politics & Government']\n",
        "\n",
        "  class_labels = {num: text for num,text in enumerate(classes)}\n",
        "\n",
        "  #tokenise sentence\n",
        "  tokenized  = [token.text for token in nlp.tokenizer(sentence)]\n",
        "\n",
        "  if len(tokenized) < min_len:\n",
        "    tokenized += ['<pad>'] * (min_len - len(tokenized))\n",
        "\n",
        "  #convert sentences to vocabulary index\n",
        "  indexed = [vocabulary.__getitem__(t) for t in tokenized]\n",
        "\n",
        "  #convert indices to tensors\n",
        "  tensor = torch.LongTensor(indexed).to(device)\n",
        "  tensor = tensor.unsqueeze(1)\n",
        "\n",
        "  #pass tensors to model to get predictions\n",
        "  preds = model(tensor)\n",
        "\n",
        "  #get index of highest value in the tensor\n",
        "  max_preds = preds.argmax(dim = 1)\n",
        "  return class_labels[max_preds.item()]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hIWWMCfDkZJC"
      },
      "source": [
        "def rank_class(model, sentence, min_len = 4, top_n = 3):\n",
        "\n",
        "  model.eval()\n",
        "\n",
        "  classes = ['Society & Culture',\n",
        "           'Science & Mathematics',\n",
        "           'Health',\n",
        "           'Education & Reference',\n",
        "           'Computers & Internet',\n",
        "           'Sports',\n",
        "           'Business & Finance',\n",
        "           'Entertainment & Music',\n",
        "           'Family & Relationships',\n",
        "           'Politics & Government']\n",
        "\n",
        "  class_labels = {num: text for num,text in enumerate(classes)}\n",
        "\n",
        "  #tokenise sentence\n",
        "  tokenized  = [token.text for token in nlp.tokenizer(sentence)]\n",
        "\n",
        "  if len(tokenized) < min_len:\n",
        "    tokenized += ['<pad>'] * (min_len - len(tokenized))\n",
        "\n",
        "  #convert sentences to vocabulary index\n",
        "  indexed = [vocabulary.__getitem__(t) for t in tokenized]\n",
        "\n",
        "  #convert indices to tensors\n",
        "  tensor = torch.LongTensor(indexed).to(device)\n",
        "  tensor = tensor.unsqueeze(1)\n",
        "\n",
        "  #pass tensors to model to get predictions\n",
        "  preds = model(tensor)\n",
        "\n",
        "  # get argsort of predictions to get ranking of predictions\n",
        "  argsort_preds = torch.argsort(preds, descending = True)\n",
        "\n",
        "  #convert tensor to list\n",
        "  argsort_list = argsort_preds.tolist()\n",
        "  argsort_list = argsort_list[0]\n",
        "\n",
        "  #return the top n most probable categories\n",
        "  top_preds = argsort_list[0:top_n]\n",
        "\n",
        "  #return the human-readable classes\n",
        "  pred_classes = [class_labels[pred] for pred in top_preds]\n",
        "\n",
        "  return pred_classes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "JiHUtdp84WIi",
        "outputId": "0012a51f-516c-4450-825d-7c5710b2aaad"
      },
      "source": [
        "predict_class(model, \"What is the square root of one hundred?\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Science & Mathematics'"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "foZE9Hfn4X-4",
        "outputId": "d71b7138-f4b1-4673-a19d-8d44465c6701"
      },
      "source": [
        "rank_class(model, \"What is the square root of one hundred?\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Science & Mathematics', 'Education & Reference', 'Business & Finance']"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    }
  ]
}